{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5de016",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-16T13:04:26.057964Z",
     "iopub.status.busy": "2024-04-16T13:04:26.057089Z",
     "iopub.status.idle": "2024-04-16T13:04:26.911910Z",
     "shell.execute_reply": "2024-04-16T13:04:26.911045Z"
    },
    "papermill": {
     "duration": 0.866452,
     "end_time": "2024-04-16T13:04:26.915133",
     "exception": false,
     "start_time": "2024-04-16T13:04:26.048681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/gemma/transformers/7b-it/2/model.safetensors.index.json\n",
      "/kaggle/input/gemma/transformers/7b-it/2/model-00003-of-00004.safetensors\n",
      "/kaggle/input/gemma/transformers/7b-it/2/config.json\n",
      "/kaggle/input/gemma/transformers/7b-it/2/gemma-7b-it.gguf\n",
      "/kaggle/input/gemma/transformers/7b-it/2/model-00001-of-00004.safetensors\n",
      "/kaggle/input/gemma/transformers/7b-it/2/tokenizer.json\n",
      "/kaggle/input/gemma/transformers/7b-it/2/tokenizer_config.json\n",
      "/kaggle/input/gemma/transformers/7b-it/2/model-00004-of-00004.safetensors\n",
      "/kaggle/input/gemma/transformers/7b-it/2/special_tokens_map.json\n",
      "/kaggle/input/gemma/transformers/7b-it/2/.gitattributes\n",
      "/kaggle/input/gemma/transformers/7b-it/2/model-00002-of-00004.safetensors\n",
      "/kaggle/input/gemma/transformers/7b-it/2/tokenizer.model\n",
      "/kaggle/input/gemma/transformers/7b-it/2/generation_config.json\n",
      "/kaggle/input/gemma/transformers/2b-it/2/model.safetensors.index.json\n",
      "/kaggle/input/gemma/transformers/2b-it/2/gemma-2b-it.gguf\n",
      "/kaggle/input/gemma/transformers/2b-it/2/config.json\n",
      "/kaggle/input/gemma/transformers/2b-it/2/model-00001-of-00002.safetensors\n",
      "/kaggle/input/gemma/transformers/2b-it/2/model-00002-of-00002.safetensors\n",
      "/kaggle/input/gemma/transformers/2b-it/2/tokenizer.json\n",
      "/kaggle/input/gemma/transformers/2b-it/2/tokenizer_config.json\n",
      "/kaggle/input/gemma/transformers/2b-it/2/special_tokens_map.json\n",
      "/kaggle/input/gemma/transformers/2b-it/2/.gitattributes\n",
      "/kaggle/input/gemma/transformers/2b-it/2/tokenizer.model\n",
      "/kaggle/input/gemma/transformers/2b-it/2/generation_config.json\n",
      "/kaggle/input/ai-mathematical-olympiad-prize/sample_submission.csv\n",
      "/kaggle/input/ai-mathematical-olympiad-prize/AIMO Prize - Note on Language and Notation.pdf\n",
      "/kaggle/input/ai-mathematical-olympiad-prize/train.csv\n",
      "/kaggle/input/ai-mathematical-olympiad-prize/test.csv\n",
      "/kaggle/input/deepseek-math/model.safetensors.index.json\n",
      "/kaggle/input/deepseek-math/model-00003-of-00003.safetensors\n",
      "/kaggle/input/deepseek-math/config.json\n",
      "/kaggle/input/deepseek-math/tokenizer.json\n",
      "/kaggle/input/deepseek-math/model-00001-of-00003.safetensors\n",
      "/kaggle/input/deepseek-math/tokenizer_config.json\n",
      "/kaggle/input/deepseek-math/model-00002-of-00003.safetensors\n",
      "/kaggle/input/deepseek-math/special_tokens_map.json\n",
      "/kaggle/input/deepseek-math/generation_config.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/config.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00002-of-00002.bin\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer_config.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model.bin.index.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/pytorch_model-00001-of-00002.bin\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/special_tokens_map.json\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/.gitattributes\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/tokenizer.model\n",
      "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1/generation_config.json\n",
      "/kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model.safetensors.index.json\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00005-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/config.json\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/consolidated.03.pt\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00006-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/consolidated.01.pt\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/consolidated.04.pt\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/consolidated.07.pt\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00009-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00008-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/consolidated.00.pt\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00013-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00016-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00014-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/consolidated.02.pt\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/tokenizer.json\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/tokenizer_config.json\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/consolidated.05.pt\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00010-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/consolidated.06.pt\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00018-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00017-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00002-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00004-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00019-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/special_tokens_map.json\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/.gitattributes\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00012-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00003-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00001-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/tokenizer.model\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00011-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00007-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/model-00015-of-00019.safetensors\n",
      "/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1/generation_config.json\n",
      "/kaggle/input/open-math-mistral/model.safetensors.index.json\n",
      "/kaggle/input/open-math-mistral/model-00003-of-00003.safetensors\n",
      "/kaggle/input/open-math-mistral/config.json\n",
      "/kaggle/input/open-math-mistral/tokenizer.json\n",
      "/kaggle/input/open-math-mistral/model-00001-of-00003.safetensors\n",
      "/kaggle/input/open-math-mistral/tokenizer_config.json\n",
      "/kaggle/input/open-math-mistral/model-00002-of-00003.safetensors\n",
      "/kaggle/input/open-math-mistral/special_tokens_map.json\n",
      "/kaggle/input/open-math-mistral/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f29fd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:04:26.929410Z",
     "iopub.status.busy": "2024-04-16T13:04:26.929023Z",
     "iopub.status.idle": "2024-04-16T13:05:04.057624Z",
     "shell.execute_reply": "2024-04-16T13:05:04.056342Z"
    },
    "papermill": {
     "duration": 37.138369,
     "end_time": "2024-04-16T13:05:04.060241",
     "exception": false,
     "start_time": "2024-04-16T13:04:26.921872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0230cc69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:05:04.075205Z",
     "iopub.status.busy": "2024-04-16T13:05:04.074913Z",
     "iopub.status.idle": "2024-04-16T13:08:29.379664Z",
     "shell.execute_reply": "2024-04-16T13:08:29.378682Z"
    },
    "papermill": {
     "duration": 205.314752,
     "end_time": "2024-04-16T13:08:29.381883",
     "exception": false,
     "start_time": "2024-04-16T13:05:04.067131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 13:05:12.533013: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-16 13:05:12.533143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-16 13:05:12.705641: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2f4408289e495299fd1fba022b6b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,  # Importing AutoModelForCausalLM for the causal language modeling task\n",
    "    AutoTokenizer,  # Importing AutoTokenizer for tokenization\n",
    "    BitsAndBytesConfig,  # Importing BitsAndBytesConfig for quantization configuration\n",
    "    AutoConfig,  # Importing AutoConfig for model configuration\n",
    "    set_seed  # Importing set_seed for reproducibility\n",
    ")\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Define the path to the model directory\n",
    "MODEL_PATH = \"/kaggle/input/deepseek-math\"\n",
    "\n",
    "# Configuration for quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Loading weights in 4-bit format\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Using non-linear quantization with 4 bits\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Using bfloat16 for computation\n",
    "    bnb_4bit_use_double_quant=True  # Using double quantization\n",
    ")\n",
    "\n",
    "# Load the model configuration\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "config.gradient_checkpointing = True  # Enable gradient checkpointing for memory optimization\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Load the pre-trained model with specific configurations\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",  # Automatically choose device placement\n",
    "    torch_dtype=\"auto\",  # Automatically choose torch data type\n",
    "    trust_remote_code=True,  # Trust remote code for loading model\n",
    "    config=config  # Set the loaded configuration\n",
    "    # quantization_config=quantization_config,  # Optionally, enable quantization with specified configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd21db05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:08:29.396924Z",
     "iopub.status.busy": "2024-04-16T13:08:29.395945Z",
     "iopub.status.idle": "2024-04-16T13:08:29.420706Z",
     "shell.execute_reply": "2024-04-16T13:08:29.419785Z"
    },
    "papermill": {
     "duration": 0.034178,
     "end_time": "2024-04-16T13:08:29.422852",
     "exception": false,
     "start_time": "2024-04-16T13:08:29.388674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>problem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000aaa</td>\n",
       "      <td>What is $1-1$?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111bbb</td>\n",
       "      <td>What is $0\\times10$?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222ccc</td>\n",
       "      <td>Solve $4+x=4$ for $x$.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                 problem\n",
       "0  000aaa          What is $1-1$?\n",
       "1  111bbb    What is $0\\times10$?\n",
       "2  222ccc  Solve $4+x=4$ for $x$."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype  # Check the data type of the model parameters\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Flag to indicate whether the dataset is private or not\n",
    "PRIVATE = True\n",
    "\n",
    "# Read the test dataset\n",
    "df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72465fd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:08:29.439425Z",
     "iopub.status.busy": "2024-04-16T13:08:29.439131Z",
     "iopub.status.idle": "2024-04-16T13:08:29.452915Z",
     "shell.execute_reply": "2024-04-16T13:08:29.452090Z"
    },
    "papermill": {
     "duration": 0.024842,
     "end_time": "2024-04-16T13:08:29.454928",
     "exception": false,
     "start_time": "2024-04-16T13:08:29.430086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229ee8</td>\n",
       "      <td>Let $k, l &gt; 0$ be parameters. The parabola $y ...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>246d26</td>\n",
       "      <td>Each of the three-digits numbers $111$ to $999...</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2fc4ad</td>\n",
       "      <td>Let the `sparkle' operation on positive intege...</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430b63</td>\n",
       "      <td>What is the minimum value of $5x^2+5y^2-8xy$ w...</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5277ed</td>\n",
       "      <td>There exists a unique increasing geometric seq...</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                            problem  answer\n",
       "0  229ee8  Let $k, l > 0$ be parameters. The parabola $y ...      52\n",
       "1  246d26  Each of the three-digits numbers $111$ to $999...     250\n",
       "2  2fc4ad  Let the `sparkle' operation on positive intege...     702\n",
       "3  430b63  What is the minimum value of $5x^2+5y^2-8xy$ w...     800\n",
       "4  5277ed  There exists a unique increasing geometric seq...     211"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the test dataset is empty, switch to the training dataset and set PRIVATE flag accordingly\n",
    "if len(df) < 5:\n",
    "    df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n",
    "    PRIVATE = False\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56f22865",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:08:29.470648Z",
     "iopub.status.busy": "2024-04-16T13:08:29.470355Z",
     "iopub.status.idle": "2024-04-16T13:08:29.474706Z",
     "shell.execute_reply": "2024-04-16T13:08:29.473732Z"
    },
    "papermill": {
     "duration": 0.015557,
     "end_time": "2024-04-16T13:08:29.477204",
     "exception": false,
     "start_time": "2024-04-16T13:08:29.461647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform garbage collection to free up memory\n",
    "import gc\n",
    "\n",
    "# Set the device for computation\n",
    "device = 'cuda'  # Using CUDA for GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7098b385",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:08:29.492680Z",
     "iopub.status.busy": "2024-04-16T13:08:29.492385Z",
     "iopub.status.idle": "2024-04-16T13:08:30.537785Z",
     "shell.execute_reply": "2024-04-16T13:08:30.536797Z"
    },
    "papermill": {
     "duration": 1.055493,
     "end_time": "2024-04-16T13:08:30.540252",
     "exception": false,
     "start_time": "2024-04-16T13:08:29.484759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def naive_parse(answer):\n",
    "    \"\"\"\n",
    "    Naive parsing function to extract numerical values from a string.\n",
    "    \n",
    "    Args:\n",
    "    - answer (str): Input string containing numerical values.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Extracted numerical values as a string.\n",
    "    \"\"\"\n",
    "    out = []  # Initialize an empty list to store extracted numerical values\n",
    "    start = False  # Flag to indicate the start of a numerical value\n",
    "    end = False  # Flag to indicate the end of a numerical value\n",
    "    for l in reversed(list(answer)):  # Iterate over characters in reverse order\n",
    "        if l in '0123456789' and not end:  # Check if the character is a digit and not already at the end\n",
    "            start = True  # Set the start flag to True\n",
    "            out.append(l)  # Append the digit to the output list\n",
    "        else:\n",
    "            if start:  # If the start flag is True\n",
    "                end = True  # Set the end flag to True\n",
    "        \n",
    "    out = reversed(out)  # Reverse the output list to get the original numerical value\n",
    "    return ''.join(out)  # Convert the list of digits to a string and return\n",
    "import transformers\n",
    "\n",
    "# Initialize a text generation pipeline using the provided model and tokenizer\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",  # Specify the task as text generation\n",
    "    model=model,  # Provide the pre-trained model\n",
    "    tokenizer=tokenizer,  # Provide the tokenizer for processing inputs\n",
    "    torch_dtype='auto',  # Automatically choose torch data type\n",
    "    device_map=\"auto\",  # Automatically choose device placement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99193328",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:08:30.557325Z",
     "iopub.status.busy": "2024-04-16T13:08:30.556459Z",
     "iopub.status.idle": "2024-04-16T13:08:30.561504Z",
     "shell.execute_reply": "2024-04-16T13:08:30.560764Z"
    },
    "papermill": {
     "duration": 0.016197,
     "end_time": "2024-04-16T13:08:30.563907",
     "exception": false,
     "start_time": "2024-04-16T13:08:30.547710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers Version: 4.39.3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "import torch\n",
    "\n",
    "# Disable memory-efficient sparse tensors for CUDA operations\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55343378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:08:30.579491Z",
     "iopub.status.busy": "2024-04-16T13:08:30.579188Z",
     "iopub.status.idle": "2024-04-16T13:08:30.589010Z",
     "shell.execute_reply": "2024-04-16T13:08:30.588179Z"
    },
    "papermill": {
     "duration": 0.019965,
     "end_time": "2024-04-16T13:08:30.590991",
     "exception": false,
     "start_time": "2024-04-16T13:08:30.571026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def process_output(output):\n",
    "    \"\"\"\n",
    "    Process the output of a mathematical expression.\n",
    "    \n",
    "    Args:\n",
    "    - output (str): Output containing mathematical expressions and possibly code blocks.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple[int, int]: Processed outputs from mathematical expressions and code blocks.\n",
    "    \"\"\"\n",
    "    result = output\n",
    "    \n",
    "    try:\n",
    "        # Extract code from the output\n",
    "        code = output.split('```')[1][7:]\n",
    "\n",
    "        # Write the extracted code to a file\n",
    "        with open('code.py', 'w') as fout:\n",
    "            fout.write(code)\n",
    "\n",
    "        # Execute the code and capture the output\n",
    "        batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "        try:\n",
    "            shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "            print(shell_output)\n",
    "            code_output = round(float(eval(shell_output))) % 1000\n",
    "        except:\n",
    "            code_output = -1\n",
    "\n",
    "        print('CODE RESULTS', code_output)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('ERROR PARSING')\n",
    "        code_output = -1\n",
    "    \n",
    "    try:\n",
    "        # Find boxed expressions in the output\n",
    "        result_output = re.findall(r'\\\\boxed\\{(.*)\\}', result)\n",
    "\n",
    "        print('BOXED', result_output)\n",
    "        # If no boxed expression is found, try naive parsing\n",
    "        if not len(result_output):\n",
    "            result_output = naive_parse(result)\n",
    "        else:\n",
    "            result_output = result_output[-1]\n",
    "\n",
    "        print('BOXED', result_output)\n",
    "        # If no result is obtained from parsing, set to -1\n",
    "        if not len(result_output):\n",
    "            result_output = -1\n",
    "        \n",
    "        else:\n",
    "            # Evaluate the expression and take modulo 1000\n",
    "            result_output = round(float(eval(result_output))) % 1000\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('ERROR PARSING')\n",
    "        result_output = -1\n",
    "    \n",
    "    return result_output, code_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdcbbe08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:08:30.606178Z",
     "iopub.status.busy": "2024-04-16T13:08:30.605940Z",
     "iopub.status.idle": "2024-04-16T13:20:05.856973Z",
     "shell.execute_reply": "2024-04-16T13:20:05.855889Z"
    },
    "papermill": {
     "duration": 695.262404,
     "end_time": "2024-04-16T13:20:05.860319",
     "exception": false,
     "start_time": "2024-04-16T13:08:30.597915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/2\n",
      "\n",
      "CODE RESULTS 38\n",
      "BOXED ['988']\n",
      "BOXED 988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:36<00:36, 36.83s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*(Mod(16 + (k - sqrt(k*(k - l + 4)))**2/(2*k**2) + (k + sqrt(k*(k - l + 4)))**2/(2*k**2), 500))\n",
      "\n",
      "CODE RESULTS -1\n",
      "BOXED ['421']\n",
      "BOXED 421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [01:04<00:00, 32.12s/it]\n",
      " 10%|█         | 1/10 [01:04<09:38, 64.29s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395\n",
      "\n",
      "CODE RESULTS 395\n",
      "BOXED ['374']\n",
      "BOXED 374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:18<00:18, 18.63s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "ERROR PARSING\n",
      "BOXED ['111']\n",
      "BOXED 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:29<00:00, 14.63s/it]\n",
      " 20%|██        | 2/10 [01:33<05:49, 43.68s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/code.py\", line 23, in <module>\n",
      "    num_special = len(special_numbers())\n",
      "  File \"/kaggle/working/code.py\", line 18, in special_numbers\n",
      "    num = sparkle_operation(num)\n",
      "  File \"/kaggle/working/code.py\", line 7, in sparkle_operation\n",
      "    return factorial(sum(int(digit) for digit in str(n)))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sympy/core/_print_helpers.py\", line 29, in __str__\n",
      "    return sstr(self, order=None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sympy/printing/printer.py\", line 372, in __call__\n",
      "    return self.__wrapped__(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sympy/printing/str.py\", line 998, in sstr\n",
      "    s = p.doprint(expr)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sympy/printing/printer.py\", line 292, in doprint\n",
      "    return self._str(self._print(expr))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sympy/printing/printer.py\", line 331, in _print\n",
      "    return printmethod(expr, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sympy/printing/str.py\", line 679, in _print_Integer\n",
      "    return str(expr.p)\n",
      "ValueError: Exceeds the limit (4300) for integer string conversion; use sys.set_int_max_str_digits() to increase the limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE RESULTS -1\n",
      "BOXED []\n",
      "BOXED 176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:19<00:19, 19.88s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/code.py\", line 21, in <module>\n",
      "    result = sparkle_numbers()\n",
      "  File \"/kaggle/working/code.py\", line 10, in sparkle_numbers\n",
      "    sparkle = factorial(sum(int(digit) for digit in str(current_number)))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sympy/core/_print_helpers.py\", line 29, in __str__\n",
      "    return sstr(self, order=None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sympy/printing/printer.py\", line 372, in __call__\n",
      "    return self.__wrapped__(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sympy/printing/str.py\", line 998, in sstr\n",
      "    s = p.doprint(expr)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sympy/printing/printer.py\", line 292, in doprint\n",
      "    return self._str(self._print(expr))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sympy/printing/printer.py\", line 331, in _print\n",
      "    return printmethod(expr, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sympy/printing/str.py\", line 679, in _print_Integer\n",
      "    return str(expr.p)\n",
      "ValueError: Exceeds the limit (4300) for integer string conversion; use sys.set_int_max_str_digits() to increase the limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE RESULTS -1\n",
      "BOXED ['155']\n",
      "BOXED 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:44<00:00, 22.17s/it]\n",
      " 30%|███       | 3/10 [02:17<05:07, 43.98s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800.0\n",
      "\n",
      "CODE RESULTS 800\n",
      "BOXED ['556}$. However, as we are asked for a non-negative modulo 1000, we need to subtract 1000 from this value to get the correct answer. Hence, the minimum value of the expression modulo 1000 is $556 - 1000 = -444$. But since we want a non-negative modulo 1000, we add 1000 to -444 to get 556. So, the minimum value of the expression modulo 1000 is 556. Therefore, the minimum value of $5x^2+5y^2-8xy$ when $x$ and $y$ range over all real numbers such that $|x-2y| + |y-2x| = 40$ is 556 modulo 1000. Hence the answer is: $\\\\boxed{556']\n",
      "BOXED 556}$. However, as we are asked for a non-negative modulo 1000, we need to subtract 1000 from this value to get the correct answer. Hence, the minimum value of the expression modulo 1000 is $556 - 1000 = -444$. But since we want a non-negative modulo 1000, we add 1000 to -444 to get 556. So, the minimum value of the expression modulo 1000 is 556. Therefore, the minimum value of $5x^2+5y^2-8xy$ when $x$ and $y$ range over all real numbers such that $|x-2y| + |y-2x| = 40$ is 556 modulo 1000. Hence the answer is: $\\boxed{556\n",
      "unmatched '}' (<string>, line 1)\n",
      "ERROR PARSING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:37<00:37, 37.05s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "ERROR PARSING\n",
      "BOXED []\n",
      "BOXED 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [02:57<00:00, 88.60s/it]\n",
      " 40%|████      | 4/10 [05:15<09:39, 96.58s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/code.py\", line 24, in <module>\n",
      "    result = geometric_sequence_sum()\n",
      "  File \"/kaggle/working/code.py\", line 12, in geometric_sequence_sum\n",
      "    solutions = solve([a*r - (10*a + b), a*r**4 - 99], (a, r))\n",
      "NameError: name 'b' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE RESULTS -1\n",
      "BOXED ['83']\n",
      "BOXED 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:55<00:55, 55.89s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/code.py\", line 33, in <module>\n",
      "    result = geometric_sequence_sum()\n",
      "  File \"/kaggle/working/code.py\", line 22, in geometric_sequence_sum\n",
      "    assert len(solutions) == 1\n",
      "AssertionError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE RESULTS -1\n",
      "BOXED ['838']\n",
      "BOXED 838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [01:25<00:00, 42.98s/it]\n",
      " 50%|█████     | 5/10 [06:41<07:43, 92.75s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_25/1958442365.py\", line 39, in <module>\n",
      "    raw_output = pipeline(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 240, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n",
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "ERROR PARSING\n",
      "BOXED []\n",
      "BOXED 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:07<00:07,  7.05s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "CODE RESULTS 1\n",
      "BOXED ['1']\n",
      "BOXED 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:39<00:00, 19.56s/it]\n",
      " 60%|██████    | 6/10 [07:20<04:58, 74.52s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "ERROR PARSING\n",
      "BOXED []\n",
      "BOXED 848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:23<00:23, 23.91s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "\n",
      "CODE RESULTS 97\n",
      "BOXED ['194}$ modulo 1000. Therefore, the answer is $\\\\boxed{194']\n",
      "BOXED 194}$ modulo 1000. Therefore, the answer is $\\boxed{194\n",
      "unmatched '}' (<string>, line 1)\n",
      "ERROR PARSING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [01:01<00:00, 30.65s/it]\n",
      " 70%|███████   | 7/10 [08:21<03:30, 70.20s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "\n",
      "CODE RESULTS 256\n",
      "BOXED ['256']\n",
      "BOXED 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:28<00:28, 28.67s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "ERROR PARSING\n",
      "BOXED []\n",
      "BOXED 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:47<00:00, 23.76s/it]\n",
      " 80%|████████  | 8/10 [09:09<02:05, 62.98s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918.000000000000\n",
      "\n",
      "CODE RESULTS 918\n",
      "BOXED ['401']\n",
      "BOXED 401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [00:32<00:32, 32.42s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n",
      "\n",
      "CODE RESULTS 437\n",
      "BOXED ['3']\n",
      "BOXED 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:57<00:00, 28.96s/it]\n",
      " 90%|█████████ | 9/10 [10:06<01:01, 61.40s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/code.py\", line 29, in <module>\n",
      "    result = find_f_of_100()\n",
      "  File \"/kaggle/working/code.py\", line 25, in find_f_of_100\n",
      "    f_100 = solve(Eq(f_f_f_100, f_100), f_n)[0]\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE RESULTS -1\n",
      "BOXED ['3', '3', '3']\n",
      "BOXED 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [01:07<01:07, 67.02s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347\n",
      "\n",
      "CODE RESULTS 347\n",
      "BOXED ['801']\n",
      "BOXED 801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [01:28<00:00, 44.14s/it]\n",
      "100%|██████████| 10/10 [11:35<00:00, 69.52s/it]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Instruction for the tool user\n",
    "tool_instruction = \" The answer should be given as a non-negative modulo 1000.\"\n",
    "tool_instruction += '\\nPlease integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.'\n",
    "\n",
    "# Determine the number of repetitions based on whether the dataset is private\n",
    "n_repetitions = 8 if PRIVATE else 2\n",
    "\n",
    "# Lists to store results and answers\n",
    "total_results = []\n",
    "total_answers = []\n",
    "\n",
    "# Iterate over the dataset\n",
    "for i in tqdm(range(len(df))):\n",
    "    id_ = df['id'].loc[i]\n",
    "    problem = df['problem'].loc[i]\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": problem + tool_instruction\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    query_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    answers = []\n",
    "    \n",
    "    # Perform multiple repetitions for robustness\n",
    "    for _ in tqdm(range(n_repetitions)):\n",
    "        try:\n",
    "            # Generate output using the text generation pipeline\n",
    "            raw_output = pipeline(\n",
    "                query_prompt, \n",
    "                max_new_tokens=2048, \n",
    "                do_sample=True, \n",
    "                temperature=0.89645,\n",
    "                return_full_text=False\n",
    "            )\n",
    "            raw_output = raw_output[0]['generated_text']\n",
    "\n",
    "            # Process the output to extract results and code outputs\n",
    "            result_output, code_output = process_output(raw_output)\n",
    "\n",
    "            # Clear CUDA memory and perform garbage collection\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            result_output, code_output = -1, -1\n",
    "        \n",
    "        results.append(result_output)\n",
    "        answers.append(code_output)\n",
    "    \n",
    "    total_results.append(results)\n",
    "    total_answers.append(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6d3b267",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:20:05.902023Z",
     "iopub.status.busy": "2024-04-16T13:20:05.901416Z",
     "iopub.status.idle": "2024-04-16T13:20:05.909118Z",
     "shell.execute_reply": "2024-04-16T13:20:05.908208Z"
    },
    "papermill": {
     "duration": 0.0311,
     "end_time": "2024-04-16T13:20:05.911273",
     "exception": false,
     "start_time": "2024-04-16T13:20:05.880173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "395\n",
      "176\n",
      "800\n",
      "83\n",
      "24\n",
      "848\n",
      "256\n",
      "918\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# List to store final answers\n",
    "final_answers = []\n",
    "\n",
    "# Iterate over the pairs of total_answers and total_results\n",
    "for a, b in zip(total_answers, total_results):\n",
    "    # Convert lists to numpy arrays\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    # Replace negative values in 'a' with corresponding values from 'b'\n",
    "    a[a < 0] = b[a < 0]\n",
    "    \n",
    "    # Count occurrences of each answer and find the most common one\n",
    "    pred = Counter(a.tolist()).most_common(2)\n",
    "\n",
    "    # Select the most common answer, excluding negative values if possible\n",
    "    ans = pred[0][0] if not pred[0][0] < 0 else pred[1][0]\n",
    "\n",
    "    # Append the final answer to the list\n",
    "    final_answers.append(ans)\n",
    "    print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dfd6719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:20:05.951938Z",
     "iopub.status.busy": "2024-04-16T13:20:05.951665Z",
     "iopub.status.idle": "2024-04-16T13:20:05.961946Z",
     "shell.execute_reply": "2024-04-16T13:20:05.961063Z"
    },
    "papermill": {
     "duration": 0.032857,
     "end_time": "2024-04-16T13:20:05.964064",
     "exception": false,
     "start_time": "2024-04-16T13:20:05.931207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229ee8</td>\n",
       "      <td>Let $k, l &gt; 0$ be parameters. The parabola $y ...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>246d26</td>\n",
       "      <td>Each of the three-digits numbers $111$ to $999...</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2fc4ad</td>\n",
       "      <td>Let the `sparkle' operation on positive intege...</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430b63</td>\n",
       "      <td>What is the minimum value of $5x^2+5y^2-8xy$ w...</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5277ed</td>\n",
       "      <td>There exists a unique increasing geometric seq...</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>739bc9</td>\n",
       "      <td>For how many positive integers $m$ does the eq...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>82e2a0</td>\n",
       "      <td>Suppose that we roll four 6-sided fair dice wi...</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8ee6f3</td>\n",
       "      <td>The points $\\left(x, y\\right)$ satisfying $((\\...</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bedda4</td>\n",
       "      <td>Let $ABCD$ be a unit square. Let $P$ be the po...</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>d7e9c9</td>\n",
       "      <td>A function $f: \\mathbb N \\to \\mathbb N$ satisf...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                            problem  answer\n",
       "0  229ee8  Let $k, l > 0$ be parameters. The parabola $y ...      38\n",
       "1  246d26  Each of the three-digits numbers $111$ to $999...     395\n",
       "2  2fc4ad  Let the `sparkle' operation on positive intege...     176\n",
       "3  430b63  What is the minimum value of $5x^2+5y^2-8xy$ w...     800\n",
       "4  5277ed  There exists a unique increasing geometric seq...      83\n",
       "5  739bc9  For how many positive integers $m$ does the eq...      24\n",
       "6  82e2a0  Suppose that we roll four 6-sided fair dice wi...     848\n",
       "7  8ee6f3  The points $\\left(x, y\\right)$ satisfying $((\\...     256\n",
       "8  bedda4  Let $ABCD$ be a unit square. Let $P$ be the po...     918\n",
       "9  d7e9c9  A function $f: \\mathbb N \\to \\mathbb N$ satisf...       3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign the final answers to the 'answer' column in the DataFrame\n",
    "df['answer'] = final_answers\n",
    "# Display the DataFrame with the updated 'answer' column\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a07d8d87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:20:06.006486Z",
     "iopub.status.busy": "2024-04-16T13:20:06.006171Z",
     "iopub.status.idle": "2024-04-16T13:20:06.032928Z",
     "shell.execute_reply": "2024-04-16T13:20:06.032005Z"
    },
    "papermill": {
     "duration": 0.04916,
     "end_time": "2024-04-16T13:20:06.034879",
     "exception": false,
     "start_time": "2024-04-16T13:20:05.985719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229ee8</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>246d26</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2fc4ad</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430b63</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5277ed</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  answer\n",
       "0  229ee8      38\n",
       "1  246d26     395\n",
       "2  2fc4ad     176\n",
       "3  430b63     800\n",
       "4  5277ed      83"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the DataFrame with 'id' and 'answer' columns to a CSV file\n",
    "df[['id', 'answer']].to_csv(\"submission.csv\", header=True, index=False)\n",
    "# Display the first few rows of 'id' and 'answer' columns\n",
    "df[['id', 'answer']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bfb1fd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:20:06.076716Z",
     "iopub.status.busy": "2024-04-16T13:20:06.076415Z",
     "iopub.status.idle": "2024-04-16T13:20:06.085611Z",
     "shell.execute_reply": "2024-04-16T13:20:06.084637Z"
    },
    "papermill": {
     "duration": 0.031703,
     "end_time": "2024-04-16T13:20:06.087509",
     "exception": false,
     "start_time": "2024-04-16T13:20:06.055806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 matches in 10 examples\n"
     ]
    }
   ],
   "source": [
    "# If the dataset is not private, evaluate model performance on the training data\n",
    "if not PRIVATE:\n",
    "    # Load the training dataset\n",
    "    df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n",
    "    \n",
    "    # Assign model predictions to the 'model_answer' column\n",
    "    df['model_answer'] = final_answers\n",
    "    \n",
    "    # Check if the model predictions match the ground truth answers\n",
    "    df['match'] = df.answer == df.model_answer\n",
    "    \n",
    "    # Calculate and print the number of matches\n",
    "    print(f'{df.match.sum()} matches in {len(df)} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b08f3b0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T13:20:06.127609Z",
     "iopub.status.busy": "2024-04-16T13:20:06.127263Z",
     "iopub.status.idle": "2024-04-16T13:20:06.235925Z",
     "shell.execute_reply": "2024-04-16T13:20:06.234895Z"
    },
    "papermill": {
     "duration": 0.130907,
     "end_time": "2024-04-16T13:20:06.237859",
     "exception": false,
     "start_time": "2024-04-16T13:20:06.106952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a simple Python script to a file\n",
    "with open('code.py', 'w') as fout:\n",
    "    fout.write(\"print('done')\")\n",
    "\n",
    "# Execute the script with a timeout using subprocess\n",
    "batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "try:\n",
    "    # Capture and print the output of the script\n",
    "    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "    print(shell_output)\n",
    "except:\n",
    "    pass  # Ignore any errors during execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5426458",
   "metadata": {
    "papermill": {
     "duration": 0.019751,
     "end_time": "2024-04-16T13:20:06.277899",
     "exception": false,
     "start_time": "2024-04-16T13:20:06.258148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea18766",
   "metadata": {
    "papermill": {
     "duration": 0.019562,
     "end_time": "2024-04-16T13:20:06.317420",
     "exception": false,
     "start_time": "2024-04-16T13:20:06.297858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677754b",
   "metadata": {
    "papermill": {
     "duration": 0.019546,
     "end_time": "2024-04-16T13:20:06.356766",
     "exception": false,
     "start_time": "2024-04-16T13:20:06.337220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a23a87f",
   "metadata": {
    "papermill": {
     "duration": 0.019383,
     "end_time": "2024-04-16T13:20:06.395622",
     "exception": false,
     "start_time": "2024-04-16T13:20:06.376239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf4cbf",
   "metadata": {
    "papermill": {
     "duration": 0.019641,
     "end_time": "2024-04-16T13:20:06.434703",
     "exception": false,
     "start_time": "2024-04-16T13:20:06.415062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8133715,
     "sourceId": 73231,
     "sourceType": "competition"
    },
    {
     "datasetId": 4281572,
     "sourceId": 7369493,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4720595,
     "sourceId": 8012825,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4728129,
     "sourceId": 8023365,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 4761,
     "sourceId": 5994,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8318,
     "sourceId": 11382,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8332,
     "sourceId": 11394,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 946.778439,
   "end_time": "2024-04-16T13:20:09.767729",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-16T13:04:22.989290",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f9d6d22a6ad4403b09e8c78a0868cf5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2c2f4408289e495299fd1fba022b6b16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c8335ef9e8c6427d8d4a26358335b495",
        "IPY_MODEL_2e9d7baea05d475099b2beb80f5d24e9",
        "IPY_MODEL_99a192bd424d4c319659472962e094a1"
       ],
       "layout": "IPY_MODEL_ffe1d7d344214c46ba48b85b561d061d"
      }
     },
     "2e9d7baea05d475099b2beb80f5d24e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_780d6627faed4b5e805b209dabfbcdaf",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3c176356aa4f4b06a77ea7642954b0a6",
       "value": 3.0
      }
     },
     "3c176356aa4f4b06a77ea7642954b0a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4d93540d35324262ab9d48e016d018a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "780d6627faed4b5e805b209dabfbcdaf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "930a766fc8054965bbe3831a232ce9de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "99a192bd424d4c319659472962e094a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4d93540d35324262ab9d48e016d018a5",
       "placeholder": "​",
       "style": "IPY_MODEL_930a766fc8054965bbe3831a232ce9de",
       "value": " 3/3 [03:05&lt;00:00, 59.97s/it]"
      }
     },
     "b1840f5f81ce417ea6925e64255f34b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c8335ef9e8c6427d8d4a26358335b495": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b1840f5f81ce417ea6925e64255f34b4",
       "placeholder": "​",
       "style": "IPY_MODEL_0f9d6d22a6ad4403b09e8c78a0868cf5",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "ffe1d7d344214c46ba48b85b561d061d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
